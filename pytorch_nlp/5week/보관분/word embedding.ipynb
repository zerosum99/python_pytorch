{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (2013)\n",
    "\n",
    "[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "by Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean\n",
    "\n",
    "![](http://i.imgur.com/agTBWiT.png)\n",
    "\n",
    "### Continuous Bag-Of-Words vs. Skip-gram\n",
    "\n",
    "* CBOW: guessing the blank\n",
    "* Skip-gram: guessing the neighbors\n",
    "\n",
    "![](https://ascelibrary.org/cms/attachment/83d45b70-be2d-4dae-a37a-e3b51af0b7c4/figure3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NANO_CORPUS = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'a', 'computational', 'process', 'computational', 'processes', 'are', 'abstract', 'beings', 'that', 'inhabit', 'computers', 'as', 'they', 'evolve', 'processes', 'manipulate', 'other', 'abstract', 'things', 'called', 'data', 'the', 'evolution', 'of', 'a', 'process', 'is', 'directed', 'by', 'a', 'pattern', 'of', 'rules', 'called', 'a', 'program', 'people', 'create', 'programs', 'to', 'direct', 'processes', 'in', 'effect', 'we', 'conjure', 'the', 'spirits', 'of', 'the', 'computer', 'with', 'our', 'spells']\n"
     ]
    }
   ],
   "source": [
    "corpus = NANO_CORPUS.lower().replace(',', ' ').replace('.', ' ').split()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dahlmoon/anaconda/envs/pytorch-nlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/dahlmoon/anaconda/envs/pytorch-nlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vocabulary = list(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin anything, we need to create a one-hot vector of the words. Pandas is great at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = torch.FloatTensor(len(vocabulary), EMBEDDING_SIZE).uniform_()\n",
    "        self.linear1 = torch.nn.Linear(EMBEDDING_SIZE, 128)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(128, len(vocabulary))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sum(self.embeddings * x.sum(dim=0).view(-1, 1), dim=0)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x.view(1, -1)\n",
    "    \n",
    "    def get_word_embedding(self, word):\n",
    "        return self.embeddings[vocabulary.index(word)].view(1, -1)\n",
    "\n",
    "cbow = CBOW()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 3.8499\n",
      "Epoch 1: loss 3.7850\n",
      "Epoch 2: loss 3.7396\n",
      "Epoch 3: loss 3.7066\n",
      "Epoch 4: loss 3.6820\n",
      "Epoch 5: loss 3.6621\n",
      "Epoch 6: loss 3.6450\n",
      "Epoch 7: loss 3.6296\n",
      "Epoch 8: loss 3.6153\n",
      "Epoch 9: loss 3.6026\n",
      "Epoch 10: loss 3.5902\n",
      "Epoch 11: loss 3.5783\n",
      "Epoch 12: loss 3.5671\n",
      "Epoch 13: loss 3.5559\n",
      "Epoch 14: loss 3.5446\n",
      "Epoch 15: loss 3.5336\n",
      "Epoch 16: loss 3.5229\n",
      "Epoch 17: loss 3.5120\n",
      "Epoch 18: loss 3.5011\n",
      "Epoch 19: loss 3.4902\n",
      "Epoch 20: loss 3.4792\n",
      "Epoch 21: loss 3.4683\n",
      "Epoch 22: loss 3.4570\n",
      "Epoch 23: loss 3.4455\n",
      "Epoch 24: loss 3.4344\n",
      "Epoch 25: loss 3.4232\n",
      "Epoch 26: loss 3.4123\n",
      "Epoch 27: loss 3.4006\n",
      "Epoch 28: loss 3.3889\n",
      "Epoch 29: loss 3.3773\n",
      "Epoch 30: loss 3.3656\n",
      "Epoch 31: loss 3.3539\n",
      "Epoch 32: loss 3.3420\n",
      "Epoch 33: loss 3.3305\n",
      "Epoch 34: loss 3.3181\n",
      "Epoch 35: loss 3.3065\n",
      "Epoch 36: loss 3.2944\n",
      "Epoch 37: loss 3.2824\n",
      "Epoch 38: loss 3.2703\n",
      "Epoch 39: loss 3.2572\n",
      "Epoch 40: loss 3.2447\n",
      "Epoch 41: loss 3.2317\n",
      "Epoch 42: loss 3.2191\n",
      "Epoch 43: loss 3.2064\n",
      "Epoch 44: loss 3.1930\n",
      "Epoch 45: loss 3.1795\n",
      "Epoch 46: loss 3.1664\n",
      "Epoch 47: loss 3.1529\n",
      "Epoch 48: loss 3.1394\n",
      "Epoch 49: loss 3.1259\n",
      "Epoch 50: loss 3.1124\n",
      "Epoch 51: loss 3.0976\n",
      "Epoch 52: loss 3.0838\n",
      "Epoch 53: loss 3.0688\n",
      "Epoch 54: loss 3.0547\n",
      "Epoch 55: loss 3.0403\n",
      "Epoch 56: loss 3.0260\n",
      "Epoch 57: loss 3.0113\n",
      "Epoch 58: loss 2.9961\n",
      "Epoch 59: loss 2.9818\n",
      "Epoch 60: loss 2.9664\n",
      "Epoch 61: loss 2.9522\n",
      "Epoch 62: loss 2.9364\n",
      "Epoch 63: loss 2.9212\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 64\n",
    "WINDOW_SIZE = 2\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "def get_context(i, corpus):\n",
    "    context = []\n",
    "    \n",
    "    start = max(i - WINDOW_SIZE, 0)\n",
    "    end = min(i + WINDOW_SIZE, len(corpus) - 1)\n",
    "    \n",
    "    for n in range(start, end):\n",
    "        if n == i:\n",
    "            continue\n",
    "        context.append(corpus[n])\n",
    "    \n",
    "    return context\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    n_words = 0\n",
    "    acc_loss = 0\n",
    "    for i, word in enumerate(corpus):\n",
    "        context = torch.FloatTensor(\n",
    "            [one_hot[word] for word in get_context(i, corpus)])\n",
    "        target = torch.LongTensor([vocabulary.index(word)])\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            output = cbow(context)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += float(loss)\n",
    "            n_words += 1\n",
    "\n",
    "    print(f'Epoch {epoch}: loss {acc_loss/n_words:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember our corpus?\n",
    "\n",
    "> We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create **programs** to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\n",
    "\n",
    "Let's see if our model can guess the highlighted word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "quiz = ['people', 'create', 'to', 'direct']\n",
    "output = cbow(torch.FloatTensor([one_hot[w] for w in quiz]))\n",
    "_, i = output.max(dim=1)\n",
    "print(vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3643, 0.9996, 0.9855, 0.7267, 0.1250, 0.0801, 0.6539, 0.8030, 0.7818,\n",
       "         0.1762, 0.4257, 0.6482, 0.1161, 0.7514, 0.8206, 0.5402, 0.2067, 0.7947,\n",
       "         0.2767, 0.5260, 0.9455, 0.9978, 0.4868, 0.7358, 0.5265, 0.7723, 0.2194,\n",
       "         0.9570, 0.6649, 0.1071, 0.0101, 0.5208, 0.3882, 0.1479, 0.1548, 0.0120,\n",
       "         0.2436, 0.2365, 0.8461, 0.6787, 0.6469, 0.6248, 0.2029, 0.8726, 0.9375,\n",
       "         0.6713, 0.7686, 0.1980, 0.6803, 0.4200, 0.0986, 0.4354, 0.4928, 0.3195,\n",
       "         0.9612, 0.0345, 0.2280, 0.6683, 0.5000, 0.3991, 0.1407, 0.9007, 0.5229,\n",
       "         0.8227, 0.4821, 0.5555, 0.3531, 0.3194, 0.5222, 0.3366, 0.3674, 0.0203,\n",
       "         0.4767, 0.9929, 0.2805, 0.3423, 0.2213, 0.0907, 0.4727, 0.2982, 0.3968,\n",
       "         0.0270, 0.1012, 0.4033, 0.0502, 0.0996, 0.9084, 0.3239, 0.1664, 0.1366,\n",
       "         0.9388, 0.1347, 0.5621, 0.2409, 0.5498, 0.0001, 0.0381, 0.4114, 0.4277,\n",
       "         0.3794, 0.2763, 0.8652, 0.3884, 0.6464, 0.1489, 0.4899, 0.9416, 0.5688,\n",
       "         0.9544, 0.9141, 0.7854, 0.4425, 0.5971, 0.9101, 0.3864, 0.3855, 0.1413,\n",
       "         0.0302, 0.5538, 0.9233, 0.4353, 0.3249, 0.3187, 0.9893, 0.2035, 0.8912,\n",
       "         0.0712, 0.0005]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.get_word_embedding('programs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embeddings = torch.FloatTensor(len(vocabulary), EMBEDDING_SIZE).normal_()\n",
    "        self.linear1 = torch.nn.Linear(EMBEDDING_SIZE, 128)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(128, len(vocabulary))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings[x]\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x.view(1, -1)\n",
    "    \n",
    "    def get_word_embedding(self, word):\n",
    "        return self.embeddings[vocabulary.index(word)].view(1, -1)\n",
    "\n",
    "skipgram = Skipgram()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(skipgram.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 3.7855\n",
      "Epoch 1: loss 3.4326\n",
      "Epoch 2: loss 3.1628\n",
      "Epoch 3: loss 2.9233\n",
      "Epoch 4: loss 2.7064\n",
      "Epoch 5: loss 2.5197\n",
      "Epoch 6: loss 2.3735\n",
      "Epoch 7: loss 2.2685\n",
      "Epoch 8: loss 2.1979\n",
      "Epoch 9: loss 2.1520\n",
      "Epoch 10: loss 2.1259\n",
      "Epoch 11: loss 2.1066\n",
      "Epoch 12: loss 2.0931\n",
      "Epoch 13: loss 2.0831\n",
      "Epoch 14: loss 2.0732\n",
      "Epoch 15: loss 2.0650\n",
      "Epoch 16: loss 2.0570\n",
      "Epoch 17: loss 2.0499\n",
      "Epoch 18: loss 2.0427\n",
      "Epoch 19: loss 2.0365\n",
      "Epoch 20: loss 2.0306\n",
      "Epoch 21: loss 2.0242\n",
      "Epoch 22: loss 2.0184\n",
      "Epoch 23: loss 2.0118\n",
      "Epoch 24: loss 2.0060\n",
      "Epoch 25: loss 1.9998\n",
      "Epoch 26: loss 1.9950\n",
      "Epoch 27: loss 1.9893\n",
      "Epoch 28: loss 1.9849\n",
      "Epoch 29: loss 1.9814\n",
      "Epoch 30: loss 1.9768\n",
      "Epoch 31: loss 1.9733\n",
      "Epoch 32: loss 1.9675\n",
      "Epoch 33: loss 1.9645\n",
      "Epoch 34: loss 1.9599\n",
      "Epoch 35: loss 1.9562\n",
      "Epoch 36: loss 1.9527\n",
      "Epoch 37: loss 1.9486\n",
      "Epoch 38: loss 1.9455\n",
      "Epoch 39: loss 1.9421\n",
      "Epoch 40: loss 1.9380\n",
      "Epoch 41: loss 1.9358\n",
      "Epoch 42: loss 1.9322\n",
      "Epoch 43: loss 1.9294\n",
      "Epoch 44: loss 1.9261\n",
      "Epoch 45: loss 1.9219\n",
      "Epoch 46: loss 1.9190\n",
      "Epoch 47: loss 1.9168\n",
      "Epoch 48: loss 1.9147\n",
      "Epoch 49: loss 1.9120\n",
      "Epoch 50: loss 1.9088\n",
      "Epoch 51: loss 1.9061\n",
      "Epoch 52: loss 1.9038\n",
      "Epoch 53: loss 1.9018\n",
      "Epoch 54: loss 1.8986\n",
      "Epoch 55: loss 1.8955\n",
      "Epoch 56: loss 1.8937\n",
      "Epoch 57: loss 1.8918\n",
      "Epoch 58: loss 1.8889\n",
      "Epoch 59: loss 1.8876\n",
      "Epoch 60: loss 1.8855\n",
      "Epoch 61: loss 1.8824\n",
      "Epoch 62: loss 1.8802\n",
      "Epoch 63: loss 1.8784\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 64\n",
    "WINDOW_SIZE = 2\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "def get_context(i, corpus):\n",
    "    context = []\n",
    "    \n",
    "    start = max(i - WINDOW_SIZE, 0)\n",
    "    end = min(i + WINDOW_SIZE, len(corpus) - 1)\n",
    "    \n",
    "    for n in range(start, end):\n",
    "        if n == i:\n",
    "            continue\n",
    "        context.append(corpus[n])\n",
    "    \n",
    "    return context\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    n_words = 0\n",
    "    acc_loss = 0\n",
    "    for i, word in enumerate(corpus):\n",
    "        center = vocabulary.index(word)\n",
    "\n",
    "        for word in get_context(i, corpus):\n",
    "            context = torch.LongTensor([vocabulary.index(word)])\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                output = skipgram(center)\n",
    "                loss = criterion(output, context)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                acc_loss += float(loss)\n",
    "                n_words += 1\n",
    "\n",
    "    print(f'Epoch {epoch}: loss {acc_loss/n_words:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people       0.152117\n",
       "things       0.031600\n",
       "to           0.021331\n",
       "computers    0.021299\n",
       "called       0.018467\n",
       "process      0.016660\n",
       "directed     0.016640\n",
       "are          0.013240\n",
       "evolve       0.010218\n",
       "is           0.010014\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_similar(query, embeddings, top_k=10):\n",
    "    embeddings = embeddings.cpu()\n",
    "    query = embeddings[vocabulary.index(query)]\n",
    "    similarity = (embeddings @ query) / (embeddings.norm() * query.norm())\n",
    "    similarity = pd.Series(dict(zip(vocabulary, similarity.numpy())))\n",
    "    similarity = similarity.sort_values(ascending=False)\n",
    "    \n",
    "    return similarity[:top_k]\n",
    "\n",
    "get_similar('people', skipgram.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe: Global Vectors for Word Representation (2014)\n",
    "\n",
    "by Jeffrey Pennington, Richard Socher, Christopher D. Manning\n",
    "\n",
    "https://www.aclweb.org/anthology/D14-1162\n",
    "\n",
    "On page 1534:\n",
    "\n",
    "> We begin with a simple example that showcases\n",
    "how certain aspects of meaning can be extracted\n",
    "directly from co-occurrence probabilities. Consider\n",
    "two words $i$ and $j$ that exhibit a particular aspect\n",
    "of interest; for concreteness, suppose we are\n",
    "interested in the concept of thermodynamic phase,\n",
    "for which we might take $i = ice$ and $j = steam$.\n",
    "The relationship of these words can be examined\n",
    "by studying the ratio of their co-occurrence probabilities\n",
    "with various probe words, $k$. For words\n",
    "$k$ related to $ice$ but not $steam$, say $k = solid$, we\n",
    "expect the ratio $Pik / Pjk$ will be large. Similarly,\n",
    "for words $k$ related to $steam$ but not $ice$, say $k =\n",
    "gas$, the ratio should be small. For words $k$ like\n",
    "$water$ or $fashion$, that are either related to both $ice$\n",
    "and $steam$, or to neither, the ratio should be close\n",
    "to one. Table 1 shows these probabilities and their\n",
    "ratios for a large corpus, and the numbers confirm\n",
    "these expectations. Compared to the raw probabilities,\n",
    "the ratio is better able to distinguish relevant\n",
    "words ($solid$ and $gas$) from irrelevant words ($water$\n",
    "and $fashion$) and it is also better able to discriminate\n",
    "between the two relevant words.\n",
    "\n",
    "$$\n",
    "\\frac{P_{solid | ice}}{P_{solid | steam}} >\n",
    "\\frac{P_{fashion | ice}}{P_{fashion | steam}} >\n",
    "\\frac{P_{gas | ice}}{P_{gas | steam}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above argument suggests that the appropriate\n",
    "starting point for word vector learning should\n",
    "be with ratios of co-occurrence probabilities rather\n",
    "than the probabilities themselves. Noting that the\n",
    "ratio $P_{ik} /P_{jk}$ depends on three words $i$, $j$, and $k$,\n",
    "the most general model takes the form,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i, w_j, \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The number of possibilities for $F$ is vast,\n",
    "but by enforcing a few desiderata we can select a\n",
    "unique choice. First, we would like $F$ to encode\n",
    "the information present the ratio $Pik / Pjk$ in the\n",
    "word vector space. Since vector spaces are inherently\n",
    "linear structures, the most natural way to do\n",
    "this is with vector differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i - w_j, \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, we note that the arguments of $F$ in Eqn. (2)\n",
    "are vectors while the right-hand side is a scalar.\n",
    "While $F$ could be taken to be a complicated function\n",
    "parameterized by, e.g., a neural network, doing\n",
    "so would obfuscate the linear structure we are\n",
    "trying to capture. To avoid this issue, we can first\n",
    "take the dot product of the arguments,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F((w_i - w_j)^T \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, note that for\n",
    "word-word co-occurrence matrices, the distinction\n",
    "between a word and a context word is arbitrary and\n",
    "that we are free to exchange the two roles. To do so\n",
    "consistently, we must not only exchange $w \\leftrightarrow \\tilde{w}$\n",
    "but also $X \\leftrightarrow X^T$. Our final model should be invariant\n",
    "under this relabeling, but Eqn. (3) is not.\n",
    "However, the symmetry can be restored in two\n",
    "steps. First, we require that $F$ be a homomorphism\n",
    "between the groups $(\\mathbb{R}, +)$ and $(\\mathbb{R}_{>0}, \\times)$, i.e.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(X-Y)=\\frac { F(X) }{ F(Y) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i^T \\tilde{w}_k - w_j^T \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i^T \\tilde{w}_k - w_j^T \\tilde{w}_k) = \\frac{F(w_i^T \\tilde{w}_k)}{F(w_j^T \\tilde{w}_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\exp(w_i^T \\tilde{w}_k - w_j^T \\tilde{w}_k) = \\frac{\\exp(w_i^T \\tilde{w}_k)}{\\exp(w_j^T \\tilde{w}_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F = \\exp$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 1533:\n",
    "> Let the matrix\n",
    "of word-word co-occurrence counts be denoted by\n",
    "$X$, whose entries $X_{ij}$ tabulate the number of times\n",
    "word $j$ occurs in the context of word $i$. Let $X_i = \\sum_k X_{ik}$\n",
    "be the number of times any word appears\n",
    "in the context of word $i$. Finally, let\n",
    "$P_{ij} = P(j|i) = X_{ij}/X_i$be the probability that word $j$ appear in the\n",
    "context of word $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i^T \\tilde{w}_k) = P_{ik} = \\frac{X_{ik}}{X_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_i^T \\tilde { w }_k =\\log { { P }_{ ik } } =\\log ({ X_{ ik })-\\log ({ X_{ i } })  }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 1535:\n",
    "> Next, we note that Eqn. (6) would exhibit the exchange\n",
    "symmetry if not for the $log(X_i)$ on the\n",
    "right-hand side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\log(X_{ik})-\\log(X_i) \\neq \\log(X_{ki})-\\log(X_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, this term is independent\n",
    "of $k$ so it can be absorbed into a bias $b_i$ for\n",
    "$w+i$. Finally, adding an additional bias $\\tilde{b}_k$ for $\\tilde{w}_k$\n",
    "restores the symmetry,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{ w }_{ i }^{ T }\\tilde { { w }_{ k } } +{ b }_{ i }+\\tilde { { b }_{ k } } =\\log ({ X_{ ik }})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the vocabulary and counting co-occurrence (again)\n",
    "\n",
    "Today's dataset, an English monolingual corpus, can be found [here](https://drive.google.com/open?id=1__lK0x_k8gtyV27QZqQUGSC4jlaQAZSC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "FILE = 'ted.en.txt'\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "vocabulary = defaultdict(int)\n",
    "co_occurrence = defaultdict(int)\n",
    "\n",
    "with open(FILE) as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split(' ')\n",
    "    for i in range(len(words)):\n",
    "        vocabulary[words[i]] += 1\n",
    "\n",
    "        for j in range(i + 1, i + WINDOW_SIZE + 1):\n",
    "            if j >= len(words):\n",
    "                break\n",
    "            keys = tuple(sorted([words[i], words[j]]))\n",
    "            co_occurrence[keys] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much words we have gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77599"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some love!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'love' in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2444"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['love']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the dictionary into a Pandas Series for convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MIN_OCCURRENCE = 10\n",
    "\n",
    "vocabulary = pd.Series(vocabulary, dtype='uint16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with the help of Pandas, let's set a minimum frequency threshold to trim the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16754"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = vocabulary[vocabulary >= MIN_OCCURRENCE]\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'love' in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_ij = np.zeros((len(vocabulary), len(vocabulary)), dtype='uint16')\n",
    "\n",
    "for (word_i, word_j), value in co_occurrence.items():\n",
    "    try:\n",
    "        i = vocabulary.index.get_loc(word_i)\n",
    "        j = vocabulary.index.get_loc(word_j)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    X_ij[i][j] = value\n",
    "    X_ij[j][i] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16754, 16754)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ij.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{ w }_{ i }^{ T }\\tilde { { w }_{ k } } +{ b }_{ i }+\\tilde { { b }_{ k } } =\\log ({ X_{ ik }})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-77b537cc07c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0macc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pytorch-nlp/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import torch\n",
    "\n",
    "DIM = 128\n",
    "ITERATIONS = 32\n",
    "X_MAX = 100\n",
    "ALPHA = 3/4\n",
    "GPU_ID = 2\n",
    "\n",
    "n_words = X_ij.shape[0]\n",
    "\n",
    "X = torch.from_numpy(X_ij.astype('float32'))\n",
    "w_main = torch.FloatTensor(n_words, DIM).uniform_(-0.5, 0.5)\n",
    "w_context = torch.FloatTensor(n_words, DIM).uniform_(-0.5, 0.5)\n",
    "b_main = torch.FloatTensor(n_words).uniform_(-0.5, 0.5)\n",
    "b_context = torch.FloatTensor(n_words).uniform_(-0.5, 0.5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    X = X.cuda(device=GPU_ID)\n",
    "    w_main = w_main.cuda(device=GPU_ID)\n",
    "    w_context = w_context.cuda(device=GPU_ID)\n",
    "    b_main = b_main.cuda(device=GPU_ID)\n",
    "    b_context = b_context.cuda(device=GPU_ID)\n",
    "\n",
    "X.requires_grad_(False)\n",
    "w_main.requires_grad_(True)\n",
    "w_context.requires_grad_(True)\n",
    "b_main.requires_grad_(True)\n",
    "b_context.requires_grad_(True)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam([w_main, w_context, b_main, b_context],\n",
    "                             lr=1e-3, weight_decay=1e-15)\n",
    "\n",
    "with torch.set_grad_enabled(True):\n",
    "    for iteration in range(ITERATIONS):\n",
    "        acc_loss = 0\n",
    "        for j in torch.randperm(n_words):\n",
    "            output = w_main @ w_context[j]\n",
    "            output += b_main\n",
    "            output += b_context[j]\n",
    "            \n",
    "            loss = criterion(output, X[:, j].log() + 1e-15)\n",
    "            \n",
    "            loss_weight = (X[:, j] / X_MAX) ** ALPHA\n",
    "            loss_weight[X[:, j] > X_MAX] = 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(loss_weight)\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_loss += float(loss.mean())\n",
    "        \n",
    "        print(f'iteration {iteration}, loss {acc_loss/n_words:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v(word):\n",
    "    i = vocabulary.index.get_loc(word)\n",
    "    return w_main[i].cpu()\n",
    "\n",
    "def analogy(target, top_k=20):\n",
    "    target /= target.norm()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        similarity = (w_main.cpu() @ target) / (w_main.cpu().norm() * target.norm())\n",
    "        similarity = pd.Series(dict(zip(vocabulary.keys(), similarity.numpy())))\n",
    "        similarity = similarity[vocabulary < 500]\n",
    "        similarity = similarity.sort_values(ascending=False)\n",
    "    \n",
    "    return similarity.sort_values(ascending=False)[:top_k]\n",
    "\n",
    "analogy(v('husband') - v('man') + v('woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy(v('heaven') - v('good') + v('bad'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization using PCA and t-SNE\n",
    "\n",
    "![](https://scontent-icn1-1.xx.fbcdn.net/v/t1.0-9/41425661_1809264752526756_3946431284045152256_n.jpg?_nc_cat=107&oh=e0b118959eaf0d6c7c97ce71b8c1136d&oe=5C20EDF1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PCA\n",
    "  * good for dimensionality reduction\n",
    "  * not always good for visualization\n",
    "  * weak against non-linear data\n",
    "* t-SNE\n",
    "  * good for visualization\n",
    "  * not so good for dimensionality reduction\n",
    "  * strong against non-linear data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FastText](https://fasttext.cc/): Library for efficient text classification and representation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "fasttext = torchtext.vocab.FastText(language='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.vectors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(vocabulary, embeddings, n_points):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    frequent = vocabulary[vocabulary < 2000].sort_values(ascending=False).index[:n_points]\n",
    "    indices = [vocabulary.index.get_loc(word) for word in frequent]\n",
    "    \n",
    "    pca = PCA(n_components=2, random_state=0)\n",
    "    with torch.no_grad():\n",
    "        results = pca.fit_transform(embeddings[indices])\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(n_points):\n",
    "        query = vocabulary.index[indices[i]]\n",
    "        x, y = results[i]\n",
    "        plt.scatter(x, y, label=query)\n",
    "        \n",
    "        # Prevent label overlapping by applying random offsets.\n",
    "        offset_x = np.random.randint(-35, 12) / 2000\n",
    "        offset_y = np.random.randint(-30, 15) / 2000\n",
    "        \n",
    "        plt.annotate(query, (x + offset_x, y + offset_y))\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "pca(vocabulary, w_main, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "    \n",
    "def tsne(vocabulary, embeddings, n_points):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    frequent = vocabulary[vocabulary < 2000].sort_values(ascending=False).index[:n_points]\n",
    "    indices = [vocabulary.index.get_loc(word) for word in frequent]\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    with torch.no_grad():\n",
    "        results = tsne.fit_transform(embeddings[indices])\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(n_points):\n",
    "        query = vocabulary.index[indices[i]]\n",
    "        x, y = results[i]\n",
    "        plt.scatter(x, y, label=query)\n",
    "        \n",
    "        # Prevent label overlapping by applying random offsets.\n",
    "        offset_x = np.random.randint(-35, 12) / 100\n",
    "        offset_y = np.random.randint(-30, 15) / 100\n",
    "        \n",
    "        plt.annotate(query, (x + offset_x, y + offset_y))\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "tsne(vocabulary, w_main, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
