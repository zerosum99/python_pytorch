{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb  \u001b[31mcnn.py\u001b[m\u001b[m*         \u001b[31mrnn.py\u001b[m\u001b[m*         \u001b[31mtrainer.py\u001b[m\u001b[m*\r\n",
      "\u001b[31m__init__.py\u001b[m\u001b[m*    cnnpy.ipynb     rnnpy.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load trainer.py\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import utils\n",
    "\n",
    "VERBOSE_SILENT = 0\n",
    "VERBOSE_EPOCH_WISE = 1\n",
    "VERBOSE_BATCH_WISE = 2\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, crit):\n",
    "        self.model = model\n",
    "        self.crit = crit\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.best = {}\n",
    "\n",
    "    def get_best_model(self):\n",
    "        self.model.load_state_dict(self.best['model'])\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def get_loss(self, y_hat, y, crit=None):\n",
    "        crit = self.crit if crit is None else crit\n",
    "        loss = crit(y_hat, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train_epoch(self, \n",
    "                    train, \n",
    "                    optimizer, \n",
    "                    batch_size=64, \n",
    "                    verbose=VERBOSE_SILENT\n",
    "                    ):\n",
    "        '''\n",
    "        Train an epoch with given train iterator and optimizer.\n",
    "        '''\n",
    "        total_loss, total_param_norm, total_grad_norm = 0, 0, 0\n",
    "        avg_loss, avg_param_norm, avg_grad_norm = 0, 0, 0\n",
    "        sample_cnt = 0\n",
    "\n",
    "        progress_bar = tqdm(train, \n",
    "                            desc='Training: ', \n",
    "                            unit='batch'\n",
    "                            ) if verbose is VERBOSE_BATCH_WISE else train\n",
    "        # Iterate whole train-set.\n",
    "        for idx, mini_batch in enumerate(progress_bar):\n",
    "            x, y = mini_batch.text, mini_batch.label\n",
    "            # Don't forget make grad zero before another back-prop.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_hat = self.model(x)\n",
    "\n",
    "            loss = self.get_loss(y_hat, y)\n",
    "            loss.backward()\n",
    "\n",
    "            total_loss += loss\n",
    "            total_param_norm += utils.get_parameter_norm(self.model.parameters())\n",
    "            total_grad_norm += utils.get_grad_norm(self.model.parameters())\n",
    "\n",
    "            # Caluclation to show status\n",
    "            avg_loss = total_loss / (idx + 1)\n",
    "            avg_param_norm = total_param_norm / (idx + 1)\n",
    "            avg_grad_norm = total_grad_norm / (idx + 1)\n",
    "\n",
    "            if verbose is VERBOSE_BATCH_WISE:\n",
    "                progress_bar.set_postfix_str('|param|=%.2f |g_param|=%.2f loss=%.4e' % (avg_param_norm,\n",
    "                                                                                        avg_grad_norm,\n",
    "                                                                                        avg_loss\n",
    "                                                                                        ))\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            sample_cnt += mini_batch.text.size(0)\n",
    "            if sample_cnt >= len(train.dataset.examples):\n",
    "                break\n",
    "\n",
    "        if verbose is VERBOSE_BATCH_WISE:\n",
    "            progress_bar.close()\n",
    "\n",
    "        return avg_loss, avg_param_norm, avg_grad_norm\n",
    "\n",
    "    def train(self, \n",
    "              train, \n",
    "              valid, \n",
    "              batch_size=64,\n",
    "              n_epochs=100, \n",
    "              early_stop=-1, \n",
    "              verbose=VERBOSE_SILENT\n",
    "              ):\n",
    "        '''\n",
    "        Train with given train and valid iterator until n_epochs.\n",
    "        If early_stop is set, \n",
    "        early stopping will be executed if the requirement is satisfied.\n",
    "        '''\n",
    "        optimizer = torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "        lowest_loss = float('Inf')\n",
    "        lowest_after = 0\n",
    "\n",
    "        progress_bar = tqdm(range(n_epochs), \n",
    "                            desc='Training: ', \n",
    "                            unit='epoch'\n",
    "                            ) if verbose is VERBOSE_EPOCH_WISE else range(n_epochs)\n",
    "        for idx in progress_bar:  # Iterate from 1 to n_epochs\n",
    "            if verbose > VERBOSE_EPOCH_WISE:\n",
    "                print('epoch: %d/%d\\tmin_valid_loss=%.4e' % (idx + 1, \n",
    "                                                             len(progress_bar), \n",
    "                                                             lowest_loss\n",
    "                                                             ))\n",
    "            avg_train_loss, avg_param_norm, avg_grad_norm = self.train_epoch(train, \n",
    "                                                                             optimizer, \n",
    "                                                                             batch_size=batch_size, \n",
    "                                                                             verbose=verbose\n",
    "                                                                             )\n",
    "            _, avg_valid_loss = self.validate(valid, \n",
    "                                              verbose=verbose\n",
    "                                              )\n",
    "\n",
    "            # Print train status with different verbosity.\n",
    "            if verbose is VERBOSE_EPOCH_WISE:\n",
    "                progress_bar.set_postfix_str('|param|=%.2f |g_param|=%.2f train_loss=%.4e valid_loss=%.4e min_valid_loss=%.4e' % (float(avg_param_norm),\n",
    "                                                                                                                                  float(avg_grad_norm),\n",
    "                                                                                                                                  float(avg_train_loss),\n",
    "                                                                                                                                  float(avg_valid_loss),\n",
    "                                                                                                                                  float(lowest_loss)\n",
    "                                                                                                                                  ))\n",
    "\n",
    "            if avg_valid_loss < lowest_loss:\n",
    "                # Update if there is an improvement.\n",
    "                lowest_loss = avg_valid_loss\n",
    "                lowest_after = 0\n",
    "\n",
    "                self.best = {'model': self.model.state_dict(),\n",
    "                             'optim': optimizer,\n",
    "                             'epoch': idx,\n",
    "                             'lowest_loss': lowest_loss\n",
    "                             }\n",
    "            else:\n",
    "                lowest_after += 1\n",
    "\n",
    "                if lowest_after >= early_stop and early_stop > 0:\n",
    "                    break\n",
    "        if verbose is VERBOSE_EPOCH_WISE:\n",
    "            progress_bar.close()\n",
    "\n",
    "    def validate(self, \n",
    "                 valid, \n",
    "                 crit=None, \n",
    "                 batch_size=256, \n",
    "                 verbose=VERBOSE_SILENT\n",
    "                 ):\n",
    "        '''\n",
    "        Validate a model with given valid iterator.\n",
    "        '''\n",
    "        # We don't need to back-prop for these operations.\n",
    "        with torch.no_grad():\n",
    "            total_loss, total_correct, sample_cnt = 0, 0, 0\n",
    "            progress_bar = tqdm(valid, \n",
    "                                desc='Validation: ', \n",
    "                                unit='batch'\n",
    "                                ) if verbose is VERBOSE_BATCH_WISE else valid\n",
    "\n",
    "            y_hats = []\n",
    "            self.model.eval()\n",
    "            # Iterate for whole valid-set.\n",
    "            for idx, mini_batch in enumerate(progress_bar):\n",
    "                x, y = mini_batch.text, mini_batch.label\n",
    "                y_hat = self.model(x)\n",
    "                # |y_hat| = (batch_size, n_classes)\n",
    "                \n",
    "                loss = self.get_loss(y_hat, y, crit)\n",
    "\n",
    "                total_loss += loss\n",
    "                sample_cnt += mini_batch.text.size(0)\n",
    "                total_correct += float(y_hat.topk(1)[1].view(-1).eq(y).sum())\n",
    "\n",
    "                avg_loss = total_loss / (idx + 1)\n",
    "                y_hats += [y_hat]\n",
    "\n",
    "                if verbose is VERBOSE_BATCH_WISE:\n",
    "                    progress_bar.set_postfix_str('valid_loss=%.4e accuarcy=%.4f' % (avg_loss, total_correct / sample_cnt))\n",
    "\n",
    "                if sample_cnt >= len(valid.dataset.examples):\n",
    "                    break\n",
    "            self.model.train()\n",
    "\n",
    "            if verbose is VERBOSE_BATCH_WISE:\n",
    "                progress_bar.close()\n",
    "\n",
    "            y_hats = torch.cat(y_hats, dim=0)\n",
    "\n",
    "            return y_hats, avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
